<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AlessandroChecco</title>
    <description></description>
    <link>http://AlessandroChecco.github.io</link>
    <atom:link
      href="http://AlessandroChecco.github.io/atom.xml" rel="self"
      type="application/rss+xml" />
    
    <item>
      <title>Modus Operandi of Crowd Workers: The Invisible Role of Microtask Work Environments</title>
      <description>&lt;html&gt;

&lt;script type=&quot;text/javascript&quot;&gt;

function detectBrowser()
{
	/*
    var fallback = true;
    
    if (navigator &amp;&amp; navigator.vendor) {
        if (navigator.vendor.search(&quot;Apple&quot;) != -1) {
            var vers = parseInt(navigator.appVersion.replace(/^.*?AppleWebKit\/(\d+).*?$/,&#39;$1&#39;),0);
                if (vers &gt;= 532) {
                    fallback = false;
                }
        }
    }
    
    var slashIndex = window.location.href.lastIndexOf(&quot;/&quot;);
    var cropped = window.location.href.slice(0,slashIndex+1);
    
    if (fallback) {
        window.location.replace( cropped + &quot;data/assets/fallback/index.html&quot; );
    }
    else {
        window.location.replace( cropped + &quot;data/assets/player/KeynoteDHTMLPlayer.html&quot; );
    }
    */
    var slashIndex = window.location.href.lastIndexOf(&quot;/&quot;);
    var cropped = window.location.href.slice(0,slashIndex+1);
    
    window.location.replace(cropped + &quot;../../../../data/assets/player/KeynoteDHTMLPlayer.html&quot;);

}

&lt;/script&gt;

&lt;body bgcolor=&quot;black&quot; onload=&quot;detectBrowser()&quot;&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
      <pubDate>
        Mon, 20 Mar 2017 00:00:00 +0000
      </pubDate>
      <link>http://AlessandroChecco.github.io/2017/03/20/modop/</link>
      <guid isPermaLink="true">http://AlessandroChecco.github.io/2017/03/20/modop/</guid>
    </item>
    
    <item>
      <title>How to Best Serve Micro-tasks to the Crowd when there Is Class Imbalance</title>
      <description>&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;does-class-inbalance-in-relevant-judgment-affects-performance&quot;&gt;Does class inbalance in relevant judgment affects performance?&lt;/h1&gt;
&lt;p&gt;We study the effect on crowd worker efficiency and effectiveness of the dominance of one class in the data they process. We aim at understanding if there is any bias in workers seeing many negative examples in the identification of positive labels.
We run comparative experiments where we measure label quality and work efficiency over different class distribution settings both including label frequency (i.e., one dominant class) as well as ordering (e.g., positive cases preceding negative ones).&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;wp-image-9418 size-full&quot; src=&quot;http://humancomputation.com/blog/wp-content/uploads/2016/10/1.png&quot; alt=&quot;batch types&quot; width=&quot;80%&quot; /&gt;&lt;br /&gt; Order of the document classes in each batch, in blue for ‘relevant’ and red for ‘non-relevant’&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We used data from TREC8.  To measure effects of class imbalance, we used two different relevant/non-relevant ratios in a batch of judging tasks: 10%-90% and 50%-50%.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;When the relevant documents are shown before the non-relevant ones we obtain the highest precision, while the worst precision is obtained when they are shown at the end of the batch.
Moreover, in batch 2 we observe a low number of true positives and a large number of false positive judgments by the workers, which shows how 90% of non-relevant documents shown at the beginning of the batch create a bias in the workers’ notion of relevance.&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;size-full wp-image-9427&quot; src=&quot;http://humancomputation.com/blog/wp-content/uploads/2016/10/VnamHK01T9yWyS1mlrzj_pic2.png&quot; alt=&quot;Mean judgment accuracy, precision and recall for each setting&quot; width=&quot;80%&quot; /&gt;&lt;br /&gt; Mean judgment accuracy, precision and recall for each setting&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;When classes are balanced, there is no statistically significant difference in the performance between different orders. On the other hand, seeing a similar number of positive and negative documents leads to good performance with more than 60% accuracy in all the three order settings.&lt;/p&gt;

&lt;h2 id=&quot;what-did-we-learn&quot;&gt;What did we learn?&lt;/h2&gt;
&lt;p&gt;When most of the documents are non-relevant and the few relevant ones are presented first, workers perform better. This is a positive result which can be easily applied in practice as in real IR evaluation settings most of the documents to be judged are non-relevant.&lt;/p&gt;

&lt;p&gt;Including in the first positions documents known to be relevant will both prime workers on relevance as well as allow for training.&lt;/p&gt;

&lt;p&gt;While in a real setting it is not possible to put relevant documents first, it would still be possible to order documents by attributes indicating their relevance (e.g., retrieval rank, number of IR systems retrieving the document, etc.) thus presenting first to the workers the documents with higher probability of being relevant.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For more, see our paper, &lt;/em&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.02171&quot;&gt;THE EFFECT OF CLASS IMBALANCE AND ORDER ON CROWDSOURCED RELEVANCE JUDGMENTS&lt;/a&gt;&lt;/p&gt;
&lt;h3 class=&quot;title mathjax&quot;&gt;&lt;em&gt;&lt;small&gt;&lt;a href=&quot;http://www.dcs.shef.ac.uk/cgi-bin/makeperson?R.Qarout&quot;&gt;Rehab K. Qarout&lt;/a&gt;, Information School, University of Sheffield&lt;/small&gt;&lt;/em&gt;
&lt;em&gt;&lt;small&gt;&lt;a href=&quot;https://scholar.google.com/citations?user=crhkrNcAAAAJ&amp;amp;hl=en&quot;&gt;Alessandro Checco&lt;/a&gt;, Information School, University of Sheffield&lt;/small&gt;&lt;/em&gt;
&lt;em&gt;&lt;small&gt;&lt;a href=&quot;https://www.sheffield.ac.uk/is/staff/demartini&quot;&gt;Gianluca Demartini&lt;/a&gt;, Information School, University of Sheffield&lt;/small&gt;&lt;/em&gt;&lt;/h3&gt;
</description>
      <pubDate>
        Sun, 19 Mar 2017 00:00:00 +0000
      </pubDate>
      <link>http://AlessandroChecco.github.io/2017/03/19/class_imbalance/</link>
      <guid isPermaLink="true">http://AlessandroChecco.github.io/2017/03/19/class_imbalance/</guid>
    </item>
    
    <item>
      <title>Pairwise, Magnitude, or Stars: What's the Best Way for Crowds to Rate?</title>
      <description>&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;is-the-ubiquitous-five-star-system-improvable&quot;&gt;Is the ubiquitous five star system improvable?&lt;/h1&gt;
&lt;p&gt;We compare three popular techniques of rating content: five star rating, pairwise comparison, and magnitude estimation.&lt;/p&gt;

&lt;p&gt;We collected 39 000 ratings on a popular crowdsourcing platform, allowing us to release a dataset that will be useful for many related studies on user rating techniques.
The&lt;strong&gt; dataset is available &lt;a href=&quot;http://github.com/AlessandroChecco/PairwiseMagnitudeStars&quot;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;
&lt;p&gt;We ask each worker to rate 10 popular paintings using 3 rating methods:&lt;/p&gt;
&lt;ul&gt;
 	&lt;li&gt;&lt;b&gt;Magnitude: &lt;/b&gt;Using any positive number (zero excluded).&lt;/li&gt;
 	&lt;li&gt;&lt;b&gt;Star: &lt;/b&gt;Choosing between 1 to 5 stars.&lt;/li&gt;
 	&lt;li&gt;&lt;b&gt;Pairwise: &lt;/b&gt;Pairwise comparisons between two images, with no ties allowed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We run 6 different experiments (one for each combination of these three types) with 100 participants in each of them. We can thus analyze the bias given by the rating system order, and the results without order bias by using the aggregated data.&lt;/p&gt;

&lt;p&gt;At the end of the rating activity in the task, we &lt;strong&gt;dynamically build&lt;/strong&gt; the three painting rankings induced by the choices of the participant, and &lt;strong&gt;ask them&lt;/strong&gt; which of the three rankings better reflects their preference (the ranking comparison is blind: There is no indication on how each ranking has been obtained, and their order is randomized).&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;size-full wp-image-9431&quot; src=&quot;http://humancomputation.com/blog/wp-content/uploads/2016/10/final.png&quot; alt=&quot;Graphical interface to let the worker express their preference on the ranking induced by their own ratings &quot; width=&quot;80%&quot; /&gt; &lt;br /&gt;Graphical interface to let the worker express their preference on the ranking induced by their own ratings.&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;whats-the-preferred-technique&quot;&gt;What’s the preferred technique?&lt;/h2&gt;
&lt;p&gt;Participants clearly &lt;em&gt;prefer the ranking obtained from their pairwise comparisons&lt;/em&gt;.  We notice a memory bias effect: The last technique used is more likely to get the most accurate description of the real user preference. Despite this, the pairwise comparison technique obtained the maximum number of preferences in all cases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://humancomputation.com/blog/wp-content/uploads/2016/10/wins.png&quot; alt=&quot;Number of expression of preference of the ranking induced by the three different techniques, grouped by the order in which the tests have been run&quot; width=&quot;80%&quot; /&gt;&lt;br /&gt; Number of expression of preference of the ranking induced by the three different techniques.&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;effort&quot;&gt;Effort&lt;/h2&gt;
&lt;p&gt;While the pairwise comparison technique clearly requires more time than the other techniques, it would be comparable in terms of time with the other techniques using a dynamic test system (of order NlogN).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://humancomputation.com/blog/wp-content/uploads/2016/10/ACFrOgBy0st8tx-qpPwScwr9u4L9DrV9KDj0IkXYdK4Yzc-bMgUB25oZ6ZGDn9aOGT6eZZN0xTEAHbcN4RxD1kBp6Fcj3aN90-j6g-nU7FVhc5piPOhj22T7SLU5cLw.png&quot; alt=&quot;Average time per test, grouped by the order in which the tests have been run&quot; width=&quot;80%&quot; /&gt;&lt;br /&gt; Average time per test.&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-did-we-learn&quot;&gt;What did we learn?&lt;/h2&gt;
&lt;ul&gt;
 	&lt;li&gt;Star rating is confirmed to be the most familiar way for users to rate content.&lt;/li&gt;
 	&lt;li&gt;Magnitude is unintuitive with no added benefit.&lt;/li&gt;
 	&lt;li&gt;Pairwise comparison, while requiring a higher number of low-effort user ratings, best reflects intrinsic user preferences and seems to be a promising alternative.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;For more, see our full paper, &lt;a href=&quot;https://arxiv.org/abs/1609.00683&quot;&gt;Pairwise, Magnitude, or Stars: What’s the Best Way for Crowds to Rate?&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3 class=&quot;title mathjax&quot;&gt;&lt;em&gt;&lt;small&gt;&lt;a href=&quot;https://scholar.google.com/citations?user=crhkrNcAAAAJ&amp;amp;hl=en&quot;&gt;Alessandro Checco&lt;/a&gt;, Information School, University of Sheffield&lt;/small&gt;&lt;/em&gt;
&lt;em&gt;&lt;small&gt;&lt;a href=&quot;https://www.sheffield.ac.uk/is/staff/demartini&quot;&gt;Gianluca Demartini&lt;/a&gt;, Information School, University of Sheffield&lt;/small&gt;&lt;/em&gt;&lt;/h3&gt;

</description>
      <pubDate>
        Sat, 18 Mar 2017 00:00:00 +0000
      </pubDate>
      <link>http://AlessandroChecco.github.io/2017/03/18/pairwise/</link>
      <guid isPermaLink="true">http://AlessandroChecco.github.io/2017/03/18/pairwise/</guid>
    </item>
    
  </channel>
</rss>
